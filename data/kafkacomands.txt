curl -i -X POST -H "Accept:application/json"  -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d '{
      "name": "aionv4-connector",
      "config": {
            "connector.class": "io.debezium.connector.mysql.MySqlConnector",
            "database.hostname": "localhost",
            "database.port": "3306",
            "database.user": "debezium",
            "database.password": "dbz",
            "database.server.id": "42",
            "database.server.name": "aionv4",
            "database.history.kafka.bootstrap.servers": "localhost:9092",
            "database.history.kafka.topic": "dbhistory_aionv4" ,
            "include.schema.changes": "true"
       }
    }'
	
bin/connect-standalone.sh config/connect-standalone.properties config/block-source.properties

bin/connect-standalone.sh config/connect-standalone.properties config/mainnet-block-source.properties

bin/kafka-console-consumer.sh --bootstrap-server localhost:9093 --topic staging.aion.block --from-beginning
	
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic dbhistory.aion

#delete a topic
bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic mainnetserver.aionv4.block

# start stop kafka/zookeeper
sudo systemctl start kafka
sudo systemctl stop kafka

# list kafka topics
bin/kafka-topics.sh --list --zookeeper localhost:2181

#
sudo nano config/connect-standalone.properties
	bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name dbhistory_aionv4 --alter --add-config retention.bytes=-1

# launch spark and bokeh server simultaneously
/usr/local/spark/spark-2.3.2-bin-hadoop2.7/bin/spark-submit --driver-memory 3g --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2 main.py

#--------------------------------------------------------------
KAFKA CONNECT SOURCE  WORKING STANDALONE CONFIG 12/14/2018
#--------------------------------------------------------------
name=mainnet-block-source
connector.class=io.debezium.connector.mysql.MySqlConnector
tasks.max=1

topic.prefix=source-aion-
mode=incrementing
increment.column.name=block_timestamp
database.server.id=1
database.server.name=mainnetserver
database.whitelist=aionv4
table.whitelist=aionv4.block
database.hostname=40.113.226.240
database.port=3306
database.user=kafka
database.password=1233tka061
database.history.kafka.bootstrap.servers=localhost:9092
database.history.kafka.topic=aion.block
include.schema.changes=false
delete.topic.enable=true
#snapshot.mode=SCHEMA_ONLY_RECOVERY
# -------------------------------------------------------------

# view last block loaded that kafka pulled from database
bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9092 --topic mainnetserver.aionv4.block

#--------------------------------------------------------------
DISTRIUBUTED WORKER SETUP
#--------------------------------------------------------------

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
# worker.properties:
bootstrap.servers=broker1:9092
broker.metadata.list=broker1:9092

# unique group id.
group.id=aion-analytics-connect-cluster

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=true
value.converter.schema.registry.url=http://localhost:8081

internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# use below section when running the connector in distributed mode
# unique offset topic.
offset.storage.topic=aion-analytics-connect-offsets
offset.storage.replication.factor=1
offset.storage.partitions=25

# unique config topic.
config.storage.topic=aion-analytics-connect-configs
config.storage.replication.factor=1

# unique status topic.
status.storage.topic=aion-analytics-connect-status
status.storage.replication.factor=1
status.storage.partitions=5

offset.flush.interval.ms=10000
rest.port=8083

# consumer config.
consumer.auto.offset.reset=earliest

offset.storage.file.filename=/opt/kafka/offsets.log
plugin.path=/usr/local/share/kafka/plugins/
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

# make topics for distributes worker setup
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic aion-analytics-connect-configs --replication-factor 1 --partitions 1 --config cleanup.policy=compact
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic aion-analytics-connect-offsets --replication-factor 1 --partitions 25 --config cleanup.policy=compact
bin/kafka-topics.sh --create --zookeeper localhost:2181 --topic aion-analytics-connect-status --replication-factor 1 --partitions 5 --config cleanup.policy=compact

# list distributed connectors
curl -H "Accept:application/json" localhost:8083/connectors/

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
*******************************************************************
# register a connector
curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ --data @config/staging-block-source.json
*******************************************************************
'{  "name": "staging-source-block",
  "config" : {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": 1,
    "mode":"timestamp",
    "timestamp.column.name":"block_timestamp",
    "group.id": "aion-analytics",
    "config.storage.topic":"aion-analytics-configs",
    "offset.storage.topic":"aion-analytics-offsets",
    "status.storage.topic":"aion-analytics-status",
    "poll.interval.ms":100,
    "database.server.id":1,
    "database.server.name": "staging",
    "database.whitelist": "aion",
    "table.whitelist": "aion.block",
    "database.hostname": "40.113.226.240",
    "database.port":"3306",
    "database.user": "kafka",
    "database.password": "1233tka061",
    "database.history.kafka.bootstrap.servers": "PLAINTEXT://:9093",
    "database.history.kafka.topic": "dbhistory.aion.block",
    "transforms": "unwrap",
    "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    "auto.offset.reset":"smallest"
  }
}'
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
******************************************************************
    aion-analytics-distributed.properties  (connect distributed worker config)
# start the aion-analytic distributed worker
bin/connect-distributed.sh config/aion-analytics-distributed.properties
*******************************************************************


bootstrap.servers=PLAINTEXT://:9093
group.id=aion-analytics

key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true

internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

offset.storage.topic=aion-analytics-offsets
offset.storage.replication.factor=1
offset.storage.partitions=25

config.storage.topic=aion-analytics-configs
config.storage.replication.factor=1

status.storage.topic=aion-analytics-status
status.storage.replication.factor=1
#status.storage.partitions=5

offset.flush.interval.ms=10000

plugin.path=/usr/local/share/kafka/plugins/
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
*******************************************************************
  distributed broker config;  server1.properties
*******************************************************************
broker.id=1
listeners=PLAINTEXT://:9093
advertised.host=localhost
advertised.listeners=PLAINTEXT://localhost:9093
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/opt/kafka/logs1
num.partitions=1
num.recovery.threads.per.data.dir=1

offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=-1
log.retention.bytes=-1
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000

zookeeper.connect=localhost:2181
zookeeper.connection.timeout.ms=6000

group.initial.rebalance.delay.ms=3
delete.topic.enable = true
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

# check connector tasks
curl -i -X GET -H "Accept:application/json" localhost:8083/connectors/staging-connector-source

# deleting a connector
curl -X DELETE localhost:8083/connectors/staging-connector-source

# stop connect worker

# restart connector
curl -X POST â€œhttp://localhost:8083/connectors/staging-connector-source/restart"

curl -X POST /connectors/staging-connector-source/restart

#list active connections
curl -X GET http://localhost:8083/connectors

# restart a connector
curl -X POST localhost:8083/connectors/connectorName/restart

# pausing a connector
curl -X PUT localhost:8083/connectors/connectorName/pause

# delete a connector
curl -X DELETE localhost:8083/connectors/staging-block-source

# resuming a connector
curl -X PUT localhost:8083/connectors/connectorName/resume

# get status of a connector
curl -s localhost:8083/connectors/connectorName/status | jq .

# get connector configuration
curl localhost:8083/connectors/connectorName | jq

# get tasks for a connector
curl -s localhost:8083/connectors/connectorName/tasks | jq .

# restarting a task
curl -X POST localhost:8083/connectors/connectorName/tasks/0/restart


# zookeeper offset delete
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --reset_offsets staging.aion.block

# list consumer groups
bin/kafka-consumer-groups.sh --list --zookeeper localhost:2181

# get information about offsets of consumer groups
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --describe --group staging-aion-transactions

# reset offsets
bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --reset-offsets --topic staging-aion-block --to-earliest --group aion-analytics

# delete offsets
bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --delete --topic staging.aion.block
bin/kafka-consumer-groups.sh --zookeeper localhost:2181 --delete --topic staging.aion.transaction

________________________________________

SPARK OFFSETS
________________________________________
# get the zookeeper spark offsets
bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9093 -topic staging.aion.block --time -1
bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9093 -topic staging.aion.transaction --time -1

# RESET offsets manually
1) Shut down the worker (aion-analytics-distributed.properties)
2) open shell:
bin/zookeeper-shell.sh localhost:2181
3) preview the result:
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --group aion-analytics --topic staging.aion.block --reset-offsets --to-earliest
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --group aion-analytics --topic staging.aion.transaction --reset-offsets --to-earliest
4) execute:
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --group aion-analytics --topic staging.aion.block --reset-offsets --to-earliest --execute
bin/kafka-consumer-groups.sh --bootstrap-server localhost:9093 --group aion-analytics --topic staging.aion.transaction --reset-offsets --to-earliest --execute


